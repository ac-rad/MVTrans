{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce97a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from importlib.machinery import SourceFileLoader\n",
    "\n",
    "from simnet.lib import datapoint, camera, transform\n",
    "from simnet.lib.net import common\n",
    "from simnet.lib.net.init.default_init import default_init\n",
    "from simnet.lib.net.dataset import extract_left_numpy_img\n",
    "from simnet.lib.net.post_processing import epnp\n",
    "from simnet.lib.net.post_processing import nms\n",
    "from simnet.lib.net.post_processing import pose_outputs as poseOut\n",
    "from simnet.lib.net.post_processing import eval3d\n",
    "from simnet.lib.net.post_processing.eval3d import measure_3d_iou, EvalMetrics, measure_ADD\n",
    "from simnet.lib.net.post_processing.segmentation_outputs import draw_segmentation_mask_gt\n",
    "from simnet.lib.net.post_processing.epnp import optimize_for_9D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698d14b6",
   "metadata": {},
   "source": [
    "# Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa87c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detection:\n",
    "    def __init__(self, camera_T_object=None, scale_matrix=None, box=None, obj_CAD=None):\n",
    "      self.camera_T_object=camera_T_object #np.ndarray\n",
    "      self.scale_matrix= scale_matrix # np.ndarray\n",
    "      self.size_label=\"small\"\n",
    "      self.ignore = False\n",
    "      self.box = box \n",
    "      self.obj_CAD=0\n",
    "    \n",
    "def get_obj_pose_and_bbox(heatmap_output, vertex_output, z_centroid_output, cov_matrices, camera_model):\n",
    "    peaks = poseOut.extract_peaks_from_centroid(np.copy(heatmap_output), max_peaks=np.inf)\n",
    "    bboxes_ext = poseOut.extract_vertices_from_peaks(np.copy(peaks), np.copy(vertex_output), np.copy(heatmap_output))  # Shape: List(np.array([8,2])) --> y,x order\n",
    "    z_centroids = poseOut.extract_z_centroid_from_peaks(np.copy(peaks), np.copy(z_centroid_output))\n",
    "    cov_matrices = poseOut.extract_cov_matrices_from_peaks(np.copy(peaks), np.copy(cov_matrices))\n",
    "    poses = []\n",
    "    for bbox_ext, z_centroid, cov_matrix, peak in zip(bboxes_ext, z_centroids, cov_matrices, peaks):\n",
    "        bbox_ext_flipped = bbox_ext[:, ::-1] # Switch from yx to xy\n",
    "        # Solve for pose up to a scale factor\n",
    "        error, camera_T_object, scale_matrix = optimize_for_9D(bbox_ext_flipped.T, camera_model, solve_for_transforms=True) \n",
    "        abs_camera_T_object, abs_object_scale = epnp.find_absolute_scale(\n",
    "            -1.0 * z_centroid, camera_T_object, scale_matrix\n",
    "        )\n",
    "        poses.append(transform.Pose(camera_T_object=abs_camera_T_object, scale_matrix=abs_object_scale))\n",
    "    return poses, bboxes_ext\n",
    "\n",
    "def get_obj_name(scene):\n",
    "    return scene[0].split(\"/\")[-3]\n",
    "\n",
    "def prune_state_dict(state_dict):\n",
    "  for key in list(state_dict.keys()):\n",
    "    state_dict[key[6:]] = state_dict.pop(key)\n",
    "  return state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15648098",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d335c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = 'PATH_TO_CHECKPOINT.ckpt'\n",
    "model_file = 'ABSOLUTE_PATH_OF_REPO/models/multiview_net.py'\n",
    "model_name = 'res_fpn'\n",
    "hparam_file= 'ABSOLUTE_PATH_OF_REPO/config/net_config_blender_multiview_2view_eval.txt'\n",
    "model_path = (model_file)\n",
    "\n",
    "parser = argparse.ArgumentParser(fromfile_prefix_chars='@')\n",
    "common.add_train_args(parser)\n",
    "hparams = parser.parse_args(['@config/net_config_blender_multiview_2view_eval.txt'])\n",
    "\n",
    "print('Using model class from:', model_path)\n",
    "net_module = SourceFileLoader(model_name, str(model_path)).load_module()\n",
    "net_attr = getattr(net_module, model_name)\n",
    "model = net_attr(hparams)\n",
    "model.apply(default_init)\n",
    "\n",
    "print('Restoring from checkpoint:', ckpt_path)\n",
    "state_dict = torch.load(ckpt_path, map_location='cuda:0')['state_dict']\n",
    "state_dict = prune_state_dict(state_dict)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a974fa5d",
   "metadata": {},
   "source": [
    "# Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81598ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from dataloader\n",
    "val_ds = datapoint.make_dataset(hparams.val_path, dataset = 'blender', multiview = True, num_multiview = hparams.num_multiview, num_samples = hparams.num_samples)\n",
    "data_loader = common.get_loader(hparams, \"val\", datapoint_dataset=val_ds)\n",
    "data = next(iter(data_loader))  \n",
    "step = 1\n",
    "obj_name = None\n",
    "step_model = 0\n",
    "\n",
    "# inference\n",
    "if hparams.network_type == 'simnet':\n",
    "    image, seg_target, depth_target, pose_targets, box_targets, keypoint_targets, _, scene_name = data\n",
    "    seg_output, depth_output, small_depth_output, pose_outputs, box_outputs, keypoint_outputs = model.forward(\n",
    "      image.cuda(), step = step_model\n",
    "  )\n",
    "    step_model +=1\n",
    "elif hparams.network_type == 'multiview':\n",
    "    image, camera_poses, camera_intrinsic, seg_target, depth_target, pose_targets, _, scene_name = data\n",
    "    camera_intrinsic=[item.cuda() for item in camera_intrinsic]\n",
    "\n",
    "    assert image.shape[1] == camera_poses.shape[1], f'dimension mismatch: num of imgs {image.shape} not equal to num of camera poses {camera_poses.shape}'\n",
    "\n",
    "    seg_output, depth_output, small_depth_output, pose_outputs, box_outputs, keypoint_outputs = model.forward(\n",
    "      imgs = image.cuda(), cam_poses = camera_poses.cuda(), cam_intr = camera_intrinsic, mode = 'val' \n",
    "  )\n",
    "else:\n",
    "    raise ValueError(f'Network type not supported: {hparams.network_type}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de8d0e5",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fce2888",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "camera_model = camera.BlenderCamera()\n",
    "with torch.no_grad():\n",
    "    left_image_np = extract_left_numpy_img(image[0], mode = 'multiview')\n",
    "    depth_vis = depth_output.get_visualization_img(np.copy(left_image_np))\n",
    "\n",
    "    depth_target[0].depth_pred=np.expand_dims(depth_target[0].depth_pred, axis=0)\n",
    "    depth_target[0].convert_to_torch_from_numpy()\n",
    "    gt_depth_vis = depth_target[0].get_visualization_img(np.copy(left_image_np))\n",
    "\n",
    "    seg_vis = seg_output.get_visualization_img(np.copy(left_image_np)) \n",
    "    seg_target[0].convert_to_numpy_from_torch()\n",
    "    gt_seg_vis = draw_segmentation_mask_gt(np.copy(left_image_np), seg_target[0].seg_pred)\n",
    "\n",
    "    c_img = cv2.cvtColor(np.array(left_image_np), cv2.COLOR_BGR2RGB)\n",
    "    pose_vis = pose_outputs.get_visualization_img(np.copy(left_image_np), camera_model=camera_model)\n",
    "    gt_pose_vis = pose_targets[0].get_visualization_img_gt(np.copy(left_image_np), camera_model=camera_model)\n",
    "\n",
    "    # plotting \n",
    "    rows = 2\n",
    "    columns = 3\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "    plt.imshow(gt_seg_vis)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"gt_seg map\")\n",
    "\n",
    "    fig.add_subplot(rows, columns, 2)\n",
    "    plt.imshow(gt_depth_vis)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"gt depth map\")\n",
    "\n",
    "    fig.add_subplot(rows, columns, 3)\n",
    "    plt.imshow(gt_pose_vis.astype(int))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"gt pose vis\")\n",
    "\n",
    "    fig.add_subplot(rows, columns, 4)\n",
    "    plt.imshow(seg_vis)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"seg map\")\n",
    "\n",
    "    fig.add_subplot(rows, columns, 5)\n",
    "    plt.imshow(depth_vis)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"depth map\")   \n",
    "\n",
    "    fig.add_subplot(rows, columns, 6)\n",
    "    plt.imshow(pose_vis.astype(int))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"pose vis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69c5d19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
